{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "reviews = pd.read_csv('./data/reviews.csv', \n",
    "                      names=[\n",
    "                          'business_id',\n",
    "                          'review_id',\n",
    "                          'user_id',\n",
    "                          'latitude',\n",
    "                          'longitude',\n",
    "                          'region',\n",
    "                          'name',\n",
    "                          'postal_code',\n",
    "                          'city',\n",
    "                          'state',\n",
    "                          'neighborhood',\n",
    "                          'text',\n",
    "                      ],\n",
    "                      dtype={\n",
    "                          'business_id': str,\n",
    "                          'review_id': str,\n",
    "                          'text': str,\n",
    "                          'user_id': str,\n",
    "                          'city': str,\n",
    "                          'latitude': np.float32,\n",
    "                          'longitude': np.float32,\n",
    "                          'region': str,\n",
    "                          'name': str,\n",
    "                          'neighborhood': str,\n",
    "                          'postal_code': str,\n",
    "                          'state': str\n",
    "                      },\n",
    "                      header=None, \n",
    "                      encoding='utf-8',\n",
    "                      nrows=100000,\n",
    "#                       skiprows=3000000,\n",
    "                      sep='|',\n",
    "                      quoting=csv.QUOTE_MINIMAL,\n",
    "                      error_bad_lines=False\n",
    "                     )\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 12)\n"
     ]
    }
   ],
   "source": [
    "reviews.dropna(subset=['text'], inplace=True)\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the above function to df['text']\n",
    "reviews['clean_text'] = reviews['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a bag of words classifier, with regions as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on just one chunk initially\n",
    "vect = CountVectorizer(stop_words='english', lowercase=True)\n",
    "counts = vect.fit_transform(reviews['clean_text'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "# train a model predicting the state from the text\n",
    "labels_train, labels_test, features_train, features_test = train_test_split(reviews['region'], tfidf, test_size=0.20, random_state=42)\n",
    "trained_model = LogisticRegression().fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78913394567\n"
     ]
    }
   ],
   "source": [
    "predictions = trained_model.predict(features_test)\n",
    "accuracy = metrics.accuracy_score(labels_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Observation 1: 78% accuracy seems alarmingly high to me, for such a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "west         67.005835\n",
      "canada       14.884074\n",
      "northeast     9.215546\n",
      "south         6.045030\n",
      "midwest       2.848514\n",
      "pnw           0.001000\n",
      "Name: region, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(labels_test.value_counts() / len(labels_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, so, maybe that explains that. The vast majority of the reviews are from the west. The majority-case classifier would perform at 67% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>west</th>\n",
       "      <th>canada</th>\n",
       "      <th>northeast</th>\n",
       "      <th>south</th>\n",
       "      <th>midwest</th>\n",
       "      <th>pnw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>west</th>\n",
       "      <td>0.779625</td>\n",
       "      <td>0.100364</td>\n",
       "      <td>0.070330</td>\n",
       "      <td>0.054514</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canada</th>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.823877</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northeast</th>\n",
       "      <td>0.072470</td>\n",
       "      <td>0.039843</td>\n",
       "      <td>0.846684</td>\n",
       "      <td>0.029302</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>0.052474</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.896763</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midwest</th>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.930044</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pnw</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               west    canada  northeast     south   midwest  pnw\n",
       "west       0.779625  0.100364   0.070330  0.054514  0.041237  NaN\n",
       "canada     0.072090  0.823877   0.027715  0.012266  0.009573  NaN\n",
       "northeast  0.072470  0.039843   0.846684  0.029302  0.013991  NaN\n",
       "south      0.052474  0.019448   0.033162  0.896763  0.005155  NaN\n",
       "midwest    0.023329  0.016467   0.022108  0.007155  0.930044  NaN\n",
       "pnw        0.000012  0.000000   0.000000  0.000000  0.000000  NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = pd.DataFrame(metrics.confusion_matrix(labels_test, predictions,\n",
    "                        labels=['west', 'canada', 'northeast', 'south', 'midwest', 'pnw']), \n",
    "                        columns=['west', 'canada', 'northeast', 'south', 'midwest', 'pnw'],\n",
    "                        index=['west', 'canada', 'northeast', 'south', 'midwest', 'pnw'])\n",
    "\n",
    "def normalize(row):\n",
    "    return row / row.sum()\n",
    "\n",
    "confusion = confusion.apply(normalize, axis=0)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not so fast! Looking at the confusion matrix, it seems this model performs well in identifying all classes! The lowest accuracy for any one class is actually the majority class, at 77%. The model correctly classifies other classes with better than 80% accuracy. This is pretty suprising to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can take a look at the top words used by the model to predict each class, and see if those give us any indication as to why the model is performing so well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['canada', 'midwest', 'northeast', 'south', 'west'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word_map = {v: k for k, v in vect.vocabulary_.items()}\n",
    "\n",
    "canada_coef = pd.Series(data=trained_model.coef_[0])\n",
    "canada_top_words_idx = canada_coef.sort_values(ascending=False)[0:50].index.values\n",
    "canada_top_words = [idx_to_word_map[idx] for idx in canada_top_words_idx]\n",
    "\n",
    "midwest_coef = pd.Series(data=trained_model.coef_[1])\n",
    "midwest_top_words_idx = midwest_coef.sort_values(ascending=False)[0:50].index.values\n",
    "midwest_top_words = [idx_to_word_map[idx] for idx in midwest_top_words_idx]\n",
    "\n",
    "northeast_coef = pd.Series(data=trained_model.coef_[2])\n",
    "northeast_top_words_idx = northeast_coef.sort_values(ascending=False)[0:50].index.values\n",
    "northeast_top_words = [idx_to_word_map[idx] for idx in northeast_top_words_idx]\n",
    "\n",
    "south_coef = pd.Series(data=trained_model.coef_[3])\n",
    "south_top_words_idx = south_coef.sort_values(ascending=False)[0:50].index.values\n",
    "south_top_words = [idx_to_word_map[idx] for idx in south_top_words_idx]\n",
    "\n",
    "west_coef = pd.Series(data=trained_model.coef_[4])\n",
    "west_top_words_idx = west_coef.sort_values(ascending=False)[0:50].index.values\n",
    "west_top_words = [idx_to_word_map[idx] for idx in west_top_words_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada        | Midwest       | Northeast     | South         |  West  \n",
      "-----------------------------------------------------------------------\n",
      "b'toronto'    | b'madison'    | b'pittsburgh' | b'charlotte'  | b'vegas'     \n",
      "b'montreal'   | b'champaign'  | b'cleveland'  | b'clt'        | b'phoenix'   \n",
      "b'flavour'    | b'wisconsin'  | b'ohio'       | b'ballantyne' | b'scottsdale'\n",
      "b'favourite'  | b'urbana'     | b'lakewood'   | b'concord'    | b'valley'    \n",
      "b'neighbou... | b'middleton'  | b'cle'        | b'noda'       | b'arizona'   \n",
      "b'canada'     | b'prairie'    | b'burgh'      | b'uptown'     | b'tempe'     \n",
      "b'flavours'   | b'curds'      | b'shadyside'  | b'matthews'   | b'chandler'  \n",
      "b'gta'        | b'capitol'    | b'pgh'        | b'huntersv... | b'casino'    \n",
      "b'flavourful' | b'illinois'   | b'oakland'    | b'gastonia'   | b'mesa'      \n",
      "b'yonge'      | b'campus'     | b'tremont'    | b'midwood'    | b'henderson' \n",
      "b'colour'     | b'badger'     | b'lawrence... | b'southpark'  | b'bellagio'  \n",
      "b'mississa... | b'fitchburg'  | b'pitt'       | b'nc'         | b'gilbert'   \n",
      "b'canadian'   | b'chambana'   | b'monroevi... | b'carolina'   | b'az'        \n",
      "b'centre'     | b'monona'     | b'eagle'      | b'dilworth'   | b'summerlin' \n",
      "b'washroom'   | b'cu'         | b'byob'       | b'teeter'     | b'phx'       \n",
      "b'markham'    | b'dane'       | b'southside'  | b'pineville'  | b'strip'     \n",
      "b'washrooms'  | b'stoughton'  | b'strongsv... | b'davidson'   | b'asu'       \n",
      "b'bloor'      | b'graze'      | b'mentor'     | b'bojangles'  | b'fremont'   \n",
      "b'kensington' | b'capital'    | b'squirrel'   | b'birkdale'   | b'wynn'      \n",
      "b'danforth'   | b'willy'      | b'penn'       | b'pimento'    | b'venetian'  \n",
      "b'poutine'    | b'verona'     | b'coventry'   | b'southend'   | b'mgm'       \n",
      "b'ontario'    | b'hilldale'   | b'akron'      | b'fort'       | b'camelback' \n",
      "b'scarboro... | b'uiuc'       | b'westlake'   | b'amelie'     | b'glendale'  \n",
      "b'yorkville'  | b'dlux'       | b'robinson'   | b'tryon'      | b'unlv'      \n",
      "b'oakville'   | b'illini'     | b'burgatory'  | b'waxhaw'     | b'mandalay'  \n",
      "b'dundas'     | b'muramoto'   | b'solon'      | b'belmont'    | b'lv'        \n",
      "b'brampton'   | b'state'      | b'avon'       | b'panthers'   | b'resort'    \n",
      "b'colours'    | b'woodman'    | b'chagrin'    | b'blvd'       | b'nevada'    \n",
      "b'favourites' | b'walleye'    | b'beachwood'  | b'epicenter'  | b'boulder'   \n",
      "b'mains'      | b'maize'      | b'cavs'       | b'northlake'  | b'las'       \n",
      "b'savoury'    | b'barriques'  | b'euclid'     | b'cowfish'    | b'arcadia'   \n",
      "b'resto'      | b'monroe'     | b'medina'     | b'cornelius'  | b'misters'   \n",
      "b'favour'     | b'milwaukee'  | b'primanti'   | b'mooresvi... | b'aria'      \n",
      "b'summerli... | b'scrambler'  | b'olmsted'    | b'131'        | b'boba'      \n",
      "b'winterli... | b'banzo'      | b'kent'       | b'epicentre'  | b'flamingo'  \n",
      "b'richmond'   | b'charter'    | b'bloomfield' | b'norman'     | b'rio'       \n",
      "b'hortons'    | b'monty'      | b'erie'       | b'ncfoodguy'  | b'safeway'   \n",
      "b'vaughan'    | b'zimbrick'   | b'symon'      | b'uncc'       | b'ahwatukee' \n",
      "b'lineup'     | b'midwest'    | b'river'      | b'publix'     | b'biltmore'  \n",
      "b'montr'      | b'brutta'     | b'hudson'     | b'sanitation' | b'palazzo'   \n",
      "b'rmt'        | b'atwood'     | b'cuyahoga'   | b'hendrick'   | b'sin'       \n",
      "b'hakka'      | b'wi'         | b'parma'      | b'harris'     | b'bouchon'   \n",
      "b'jays'       | b'rangoon'    | b'cmu'        | b'carowinds'  | b'peaks'     \n",
      "b'spadina'    | b'ian'        | b'mayfield'   | b'montford'   | b'chile'     \n",
      "b'flavoured'  | b'courier'    | b'darren'     | b'grits'      | b'grimaldi'  \n",
      "b'flavourl... | b'za'         | b'pittsburg'  | b'woudl'      | b'yellowtail'\n",
      "b'peameal'    | b'sakanaya'   | b'pennsylv... | b'stonecrest' | b'cosmopol...\n",
      "b'mtl'        | b'greenbush'  | b'hoagies'    | b'omb'        | b'panda'     \n",
      "b'yorkdale'   | b'badgers'    | b'pnc'        | b'flounder'   | b'peoria'    \n",
      "b'ossington'  | b'thorps'     | b'pirates'    | b'harrisburg' | b'sahara'    \n"
     ]
    }
   ],
   "source": [
    "def clip(string):\n",
    "    return string[:10] + '...' if len(string) > 13 else string\n",
    "\n",
    "print('Canada        | Midwest       | Northeast     | South         |  West  ')\n",
    "print('-----------------------------------------------------------------------')\n",
    "for i in range(len(canada_top_words)):\n",
    "    print('{0} | {1} | {2} | {3} | {4}'.format(\n",
    "        clip(str(canada_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(midwest_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(northeast_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(south_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(west_top_words[i].encode('utf-8'))).ljust(13)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 2: This gives us a good idea of what is going on. The model does a good job of keying off of \"giveaway\" words that strongly indicate the location. These words include proper nouns like city names, landmarks, and companies. They also include nicknames or abbreviations for major landmarks, and even a sports team 'cavs'.  But not all is lost. Especially in the Canadian column, we can see some words that could be considered indicative of dialect, like 'flavour' and 'centre'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 3: Maybe the model thinks these things are great indicators b/c they don't appear in other classes, but in reality they just don't occur that much at all. Should the model more heavily weight common words? Maybe tf-idf is a terrible call here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 4: This model might really be overfitting - we should try it against another data set, and/or try applying regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain baseline without proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=<>]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 rows\n",
      "Completed 10000 rows\n",
      "Completed 20000 rows\n",
      "Completed 30000 rows\n",
      "Completed 40000 rows\n",
      "Completed 50000 rows\n",
      "Completed 60000 rows\n",
      "Completed 70000 rows\n",
      "Completed 80000 rows\n",
      "Completed 90000 rows\n",
      "CPU times: user 1min 53s, sys: 1.08 s, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sents = []\n",
    "for i, review in enumerate(reviews['text'].values):\n",
    "    sents.append(nltk.word_tokenize(review))\n",
    "    if i % 10000 == 0:\n",
    "        print('Completed {0} rows'.format(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10000 rows\n",
      "Completed 20000 rows\n",
      "Completed 30000 rows\n",
      "Completed 40000 rows\n",
      "Completed 50000 rows\n",
      "Completed 60000 rows\n",
      "Completed 70000 rows\n",
      "Completed 80000 rows\n",
      "Completed 90000 rows\n",
      "Completed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for i in range(0, len(sents), 10000):\n",
    "    end = i+10000\n",
    "    if end > len(sents):\n",
    "        end = len(sents)\n",
    "    batch = sents[i:end]\n",
    "    tags.extend(nltk.pos_tag_sents(batch))\n",
    "    print('Completed {0} rows'.format(str(i+10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "cleaned_sents = []\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "for i, review in enumerate(tags):\n",
    "    \n",
    "    ## Remove stop words\n",
    "    review = [w for w in review if not w[0].lower() in stops and len(w[0]) >= 3]\n",
    "    \n",
    "    ## Replace proper nouns with <NNP>\n",
    "    review = [w[0] if not (w[1] in ['NNP', 'NNPS']) else '<NNP>' for w in review]\n",
    "    \n",
    "    cleaned_sents.append(clean_text(\" \".join(review)))\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0W4lkclzZThpx3V65bVgig</td>\n",
       "      <td>v0i_UHJMo_hPBq9bxWvW4w</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>45.516373</td>\n",
       "      <td>-73.577538</td>\n",
       "      <td>canada</td>\n",
       "      <td>Schwartz's</td>\n",
       "      <td>H2W 1X9</td>\n",
       "      <td>Montréal</td>\n",
       "      <td>QC</td>\n",
       "      <td>Plateau-Mont-Royal</td>\n",
       "      <td>Love the staff, love the meat, love the place....</td>\n",
       "      <td>Love staff love meat love place &lt;NNP&gt; long lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eZDXz_RylvdD0tHEA8I0NA</td>\n",
       "      <td>T2cqOo7zPjaPtxdHFeZn8w</td>\n",
       "      <td>CKRfBUqQGaVCYTKN5kDrzw</td>\n",
       "      <td>35.140625</td>\n",
       "      <td>-80.737640</td>\n",
       "      <td>south</td>\n",
       "      <td>New Zealand Cafe</td>\n",
       "      <td>28270</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good sushi for good prices! I tried the Dancin...</td>\n",
       "      <td>Love staff love meat love place &lt;NNP&gt; long lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>45.523335</td>\n",
       "      <td>-73.594856</td>\n",
       "      <td>canada</td>\n",
       "      <td>Wilensky's</td>\n",
       "      <td>H2T 2M1</td>\n",
       "      <td>Montréal</td>\n",
       "      <td>QC</td>\n",
       "      <td>Plateau-Mont-Royal</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>&lt;NNP&gt; simple place amazing nonetheless around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rjkda__H64ILwIvaccVyLg</td>\n",
       "      <td>Da9HY5-ZKdBbStAs-Ju4YA</td>\n",
       "      <td>CKRfBUqQGaVCYTKN5kDrzw</td>\n",
       "      <td>35.212841</td>\n",
       "      <td>-80.858803</td>\n",
       "      <td>south</td>\n",
       "      <td>Blaze Fast-Fire'd Pizza</td>\n",
       "      <td>28203</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>South End</td>\n",
       "      <td>Marvelous pizza! I had a make your own pizza w...</td>\n",
       "      <td>&lt;NNP&gt; simple place amazing nonetheless around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VR6GpWIda3SfvPC-lg9H3w</td>\n",
       "      <td>n6QzIUObkYshz4dz2QRJTw</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>45.472900</td>\n",
       "      <td>-73.588318</td>\n",
       "      <td>canada</td>\n",
       "      <td>Tuck Shop</td>\n",
       "      <td>H4C 1S7</td>\n",
       "      <td>Montréal</td>\n",
       "      <td>QC</td>\n",
       "      <td>Sud-Ouest</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "      <td>Small unassuming place changes menu every ofte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id               review_id                 user_id  \\\n",
       "0  0W4lkclzZThpx3V65bVgig  v0i_UHJMo_hPBq9bxWvW4w  bv2nCi5Qv5vroFiqKGopiw   \n",
       "0  eZDXz_RylvdD0tHEA8I0NA  T2cqOo7zPjaPtxdHFeZn8w  CKRfBUqQGaVCYTKN5kDrzw   \n",
       "1  AEx2SYEUJmTxVVB18LlCwA  vkVSCC7xljjrAI4UGfnKEQ  bv2nCi5Qv5vroFiqKGopiw   \n",
       "1  Rjkda__H64ILwIvaccVyLg  Da9HY5-ZKdBbStAs-Ju4YA  CKRfBUqQGaVCYTKN5kDrzw   \n",
       "2  VR6GpWIda3SfvPC-lg9H3w  n6QzIUObkYshz4dz2QRJTw  bv2nCi5Qv5vroFiqKGopiw   \n",
       "\n",
       "    latitude  longitude  region                     name postal_code  \\\n",
       "0  45.516373 -73.577538  canada               Schwartz's     H2W 1X9   \n",
       "0  35.140625 -80.737640   south         New Zealand Cafe       28270   \n",
       "1  45.523335 -73.594856  canada               Wilensky's     H2T 2M1   \n",
       "1  35.212841 -80.858803   south  Blaze Fast-Fire'd Pizza       28203   \n",
       "2  45.472900 -73.588318  canada                Tuck Shop     H4C 1S7   \n",
       "\n",
       "        city state        neighborhood  \\\n",
       "0   Montréal    QC  Plateau-Mont-Royal   \n",
       "0  Charlotte    NC                 NaN   \n",
       "1   Montréal    QC  Plateau-Mont-Royal   \n",
       "1  Charlotte    NC           South End   \n",
       "2   Montréal    QC           Sud-Ouest   \n",
       "\n",
       "                                                text  \\\n",
       "0  Love the staff, love the meat, love the place....   \n",
       "0  Good sushi for good prices! I tried the Dancin...   \n",
       "1  Super simple place but amazing nonetheless. It...   \n",
       "1  Marvelous pizza! I had a make your own pizza w...   \n",
       "2  Small unassuming place that changes their menu...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Love staff love meat love place <NNP> long lin...  \n",
       "0  Love staff love meat love place <NNP> long lin...  \n",
       "1  <NNP> simple place amazing nonetheless around ...  \n",
       "1  <NNP> simple place amazing nonetheless around ...  \n",
       "2  Small unassuming place changes menu every ofte...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews.join(pd.DataFrame(cleaned_sents, columns=['clean_text']))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on just one chunk initially\n",
    "vect = CountVectorizer(stop_words='english', lowercase=True)\n",
    "counts = vect.fit_transform(reviews['clean_text'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "# train a model predicting the state from the text\n",
    "labels_train, labels_test, features_train, features_test = train_test_split(reviews['region'], tfidf, test_size=0.20, random_state=42)\n",
    "trained_model = LogisticRegression().fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6701\n"
     ]
    }
   ],
   "source": [
    "predictions = trained_model.predict(features_test)\n",
    "accuracy = metrics.accuracy_score(labels_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "west         66.945\n",
      "canada       14.995\n",
      "northeast     8.880\n",
      "south         6.215\n",
      "midwest       2.965\n",
      "Name: region, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(labels_test.value_counts() / len(labels_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word_map = {v: k for k, v in vect.vocabulary_.items()}\n",
    "\n",
    "canada_coef = pd.Series(data=trained_model.coef_[0])\n",
    "canada_top_words_idx = canada_coef.sort_values(ascending=False)[0:50].index.values\n",
    "canada_top_words = [idx_to_word_map[idx] for idx in canada_top_words_idx]\n",
    "\n",
    "midwest_coef = pd.Series(data=trained_model.coef_[1])\n",
    "midwest_top_words_idx = midwest_coef.sort_values(ascending=False)[0:50].index.values\n",
    "midwest_top_words = [idx_to_word_map[idx] for idx in midwest_top_words_idx]\n",
    "\n",
    "northeast_coef = pd.Series(data=trained_model.coef_[2])\n",
    "northeast_top_words_idx = northeast_coef.sort_values(ascending=False)[0:50].index.values\n",
    "northeast_top_words = [idx_to_word_map[idx] for idx in northeast_top_words_idx]\n",
    "\n",
    "south_coef = pd.Series(data=trained_model.coef_[3])\n",
    "south_top_words_idx = south_coef.sort_values(ascending=False)[0:50].index.values\n",
    "south_top_words = [idx_to_word_map[idx] for idx in south_top_words_idx]\n",
    "\n",
    "west_coef = pd.Series(data=trained_model.coef_[4])\n",
    "west_top_words_idx = west_coef.sort_values(ascending=False)[0:50].index.values\n",
    "west_top_words = [idx_to_word_map[idx] for idx in west_top_words_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada        | Midwest       | Northeast     | South         |  West  \n",
      "-----------------------------------------------------------------------\n",
      "b'repeat'     | b'stop'       | b'emails'     | b'descript... | b'pissed'    \n",
      "b'describes'  | b'vegan'      | b'vermicelli' | b'gluten'     | b'hat'       \n",
      "b'decide'     | b'insurance'  | b'liars'      | b'cent'       | b'companion' \n",
      "b'le'         | b'easily'     | b'easier'     | b'parking'    | b'lacked'    \n",
      "b'fingers'    | b'friend'     | b'oxtail'     | b'husbands'   | b'billing'   \n",
      "b'flies'      | b'delivered'  | b'correct'    | b'sold'       | b'overpowe...\n",
      "b'shwarma'    | b'door'       | b'program'    | b'upgraded'   | b'entrees'   \n",
      "b'photo'      | b'accident... | b'hve'        | b'sea'        | b'empanadas' \n",
      "b'ian'        | b'walking'    | b'mid'        | b'event'      | b'checks'    \n",
      "b'lait'       | b'kidding'    | b'odd'        | b'greeted'    | b'scheduling'\n",
      "b'wan'        | b'yummy'      | b'hell'       | b'uptown'     | b'pregnant'  \n",
      "b'laptop'     | b'miles'      | b'acidic'     | b'sister'     | b'discounted'\n",
      "b'yeah'       | b'tanning'    | b'velvet'     | b'contract'   | b'salted'    \n",
      "b'meant'      | b'unlike'     | b'cracker'    | b'invited'    | b'venetian'  \n",
      "b'artist'     | b'greatest'   | b'homemade'   | b'rly'        | b'tamales'   \n",
      "b'talked'     | b'forget'     | b'pros'       | b'accomoda... | b'fruity'    \n",
      "b'succulent'  | b'incredibly' | b'focused'    | b'piece'      | b'picky'     \n",
      "b'des'        | b'tournament' | b'doughnuts'  | b'electronic' | b'snap'      \n",
      "b'pics'       | b'bartender'  | b'chat'       | b'215'        | b'hardly'    \n",
      "b'strongly'   | b'fast'       | b'meet'       | b'boyfriend'  | b'constant'  \n",
      "b'turnover'   | b'steep'      | b'community'  | b'ring'       | b'fusion'    \n",
      "b'yumm'       | b'unique'     | b'skipped'    | b'sports'     | b'hangout'   \n",
      "b'chocolat'   | b'tank'       | b'planet'     | b'truly'      | b'narrow'    \n",
      "b'criticism'  | b'staff'      | b'happens'    | b'city'       | b'deirdra'   \n",
      "b'terms'      | b'keeping'    | b'cirque'     | b'internet'   | b'connected' \n",
      "b'vaut'       | b'terri'      | b'importan... | b'wrapped'    | b'pinball'   \n",
      "b'generously' | b'waits'      | b'world'      | b'visible'    | b'plus'      \n",
      "b'dishwasher' | b'offer'      | b'crap'       | b'tapas'      | b'plantains' \n",
      "b'barbershop' | b'lot'        | b'jacket'     | b'ruin'       | b'clue'      \n",
      "b'snob'       | b'matter'     | b'mugs'       | b'voucher'    | b'fyi'       \n",
      "b'advise'     | b'dark'       | b'cleanlin... | b'macaroni'   | b'bridal'    \n",
      "b'locals'     | b'dirty'      | b'dead'       | b'purple'     | b'burned'    \n",
      "b'est'        | b'knew'       | b'levels'     | b'tiramisu'   | b'pomegran...\n",
      "b'izakaya'    | b'products'   | b'speak'      | b'cooks'      | b'offerings' \n",
      "b'portabello' | b'beds'       | b'spiced'     | b'21st'       | b'brazilian' \n",
      "b'memorable'  | b'10'         | b'chicks'     | b'premium'    | b'saut'      \n",
      "b'que'        | b'tavern'     | b'arrangem... | b'trees'      | b'shitty'    \n",
      "b'described'  | b'big'        | b'shellfish'  | b'hipster'    | b'volunteer' \n",
      "b'creek'      | b'procedures' | b'dry'        | b'communic... | b'crepes'    \n",
      "b'prawns'     | b'selling'    | b'advance'    | b'trays'      | b'landscape' \n",
      "b'unlikely'   | b'posts'      | b'barbecue'   | b'trendy'     | b'frequent'  \n",
      "b'figure'     | b'annivers... | b'decades'    | b'fees'       | b'gaming'    \n",
      "b'flavours'   | b'sing'       | b'drinking'   | b'chipotle'   | b'acknowle...\n",
      "b'pretenti... | b'apart'      | b'mood'       | b'oh'         | b'couches'   \n",
      "b'courteous'  | b'exceptio... | b'tired'      | b'unhappy'    | b'vegetables'\n",
      "b'cancel'     | b'prompt'     | b'ride'       | b'hope'       | b'runny'     \n",
      "b'cooks'      | b'combo'      | b'pok'        | b'pro'        | b'margaritas'\n",
      "b'inspired'   | b'hesitant'   | b'butters'    | b'goods'      | b'teppanyaki'\n",
      "b'sleep'      | b'24'         | b'stones'     | b'tourists'   | b'rose'      \n",
      "b'extensive'  | b'smothered'  | b'lunches'    | b'beers'      | b'hip'       \n"
     ]
    }
   ],
   "source": [
    "def clip(string):\n",
    "    return string[:10] + '...' if len(string) > 13 else string\n",
    "\n",
    "print('Canada        | Midwest       | Northeast     | South         |  West  ')\n",
    "print('-----------------------------------------------------------------------')\n",
    "for i in range(len(canada_top_words)):\n",
    "    print('{0} | {1} | {2} | {3} | {4}'.format(\n",
    "        clip(str(canada_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(midwest_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(northeast_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(south_top_words[i].encode('utf-8'))).ljust(13),\n",
    "        clip(str(west_top_words[i].encode('utf-8'))).ljust(13)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
