{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll try out an LSTM on the same data as our baseline model, and see how it performs. Our hypothesis is that it won't perform any better, yet, becuase the baseline does such a good job at keying off pronouns to do dialect classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "reviews = pd.read_csv('./data/reviews.csv', \n",
    "                      names=[\n",
    "                          'business_id',\n",
    "                          'review_id',\n",
    "                          'user_id',\n",
    "                          'latitude',\n",
    "                          'longitude',\n",
    "                          'region',\n",
    "                          'name',\n",
    "                          'postal_code',\n",
    "                          'city',\n",
    "                          'state',\n",
    "                          'neighborhood',\n",
    "                          'text',\n",
    "                      ],\n",
    "                      dtype={\n",
    "                          'business_id': str,\n",
    "                          'review_id': str,\n",
    "                          'text': str,\n",
    "                          'user_id': str,\n",
    "                          'city': str,\n",
    "                          'latitude': np.float32,\n",
    "                          'longitude': np.float32,\n",
    "                          'region': str,\n",
    "                          'name': str,\n",
    "                          'neighborhood': str,\n",
    "                          'postal_code': str,\n",
    "                          'state': str\n",
    "                      },\n",
    "                      header=None, \n",
    "                      encoding='utf-8',\n",
    "                      nrows=1000000,\n",
    "#                       skiprows=3000000,\n",
    "                      sep='|',\n",
    "                      quoting=csv.QUOTE_MINIMAL,\n",
    "                      error_bad_lines=False\n",
    "                     )\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999992, 12)\n"
     ]
    }
   ],
   "source": [
    "reviews.dropna(subset=['text'], inplace=True)\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000 # only includes 20K most frequently occurring words\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(reviews['text'])\n",
    "sequences = tokenizer.texts_to_sequences(reviews['text'])\n",
    "\n",
    "# Longest review is 589 words, tutorial reduced to 50\n",
    "data = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['region'] = reviews.region.astype('category')\n",
    "categorical_labels = to_categorical(reviews['region'].cat.codes, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train, labels_test, features_train, features_test = train_test_split(categorical_labels, data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture, default batch_size is 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=511))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Fit the model\n",
    "model.fit(features_train, labels_train, validation_split=0.4, epochs=3)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(features_test, labels_test)\n",
    "print(score)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
